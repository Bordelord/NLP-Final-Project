{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names: Jose Mazariegos & Cameron Knopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that execution of this cell doesn't return any errors. If it does, go the class repository and follow the environment setup instructions\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk import word_tokenize\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (str):\n",
    "    Returns: a list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "    ### YOUR CODE BELOW ###\n",
    "    tokens_with_punct = word_tokenize(data)\n",
    "    tokens = [token for token in tokens_with_punct if token.isalpha()] # remove punctuation from tokens\n",
    "    ### YOUR CODE ABOVE ###\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            ### YOUR CODE BELOW ###\n",
    "            self.w2cnt[token]+=1   # increment count of given token\n",
    "            if token not in self.w2idx:\n",
    "                new_index = self.__len__()\n",
    "                self.idx2w[new_index] = token\n",
    "                self.w2idx[token] = new_index\n",
    "            \n",
    "            ### YOUR CODE ABOVE ###\n",
    "\n",
    "    def add_token(self, token):\n",
    "        ### YOUR CODE BELOW ###\n",
    "        self.w2cnt[token]+=1   # increment count of given token\n",
    "        if token not in self.w2idx:\n",
    "            new_index = self.__len__()\n",
    "            self.idx2w[new_index] = token\n",
    "            self.w2idx[token] = new_index\n",
    "        ### YOUR CODE ABOVE ###\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        # do not forget to update the self.w2idx and self.idx2w dictionaries\n",
    "        ### YOUR CODE BELOW ###\n",
    "        for index in list(self.w2cnt):\n",
    "            if self.w2cnt[index]<=2:\n",
    "                del self.w2cnt[index]\n",
    "                tok = self.w2idx[index] # get corresponding token for given word\n",
    "                del self.w2idx[index]\n",
    "                del self.idx2w[tok]\n",
    "        ### YOUR CODE ABOVE ###\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Need to fix _generate_pairs\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.skip_window = skip_window\n",
    "\n",
    "        self.pairs = self._generate_pairs(data, skip_window)\n",
    "\n",
    "    def _generate_pairs(self, data, skip_window):\n",
    "        \"\"\"\n",
    "        Args: input data (a list of tokens) and the window size\n",
    "        Returns: all possible pairs for the SkipGram mode\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "\n",
    "        # do not forget to filter out pairs with out-of-vocabulary tokens \n",
    "        ### YOUR CODE BELOW ###\n",
    "        \n",
    "        # used this site as a reference for this part https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "        \n",
    "        indices = []\n",
    "        for word in data:\n",
    "            if word in self.vocab.w2idx:\n",
    "                indices.append(self.vocab.w2idx[word])\n",
    "        maxIdx = len(indices) - 1\n",
    "        \n",
    "        \n",
    "        for centerIdx in range(1, len(indices)):\n",
    "            \n",
    "            for windowIdx in range(-skip_window, skip_window+1):\n",
    "                contextIdx = centerIdx + windowIdx\n",
    "                \n",
    "                if contextIdx < 0 or contextIdx > maxIdx or contextIdx == centerIdx:\n",
    "                    continue\n",
    "                if self.vocab.w2cnt[self.vocab.idx2w[indices[centerIdx]]] == 0:\n",
    "                    continue\n",
    "                if self.vocab.w2cnt[self.vocab.idx2w[indices[contextIdx]]] == 0:\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "                contextWord = self.vocab.idx2w[indices[centerIdx]]\n",
    "                centerWord = self.vocab.idx2w[indices[contextIdx]]\n",
    "                \n",
    "                pairs.append((centerWord, contextWord))\n",
    "                \n",
    "        return pairs\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        ### YOUR CODE BELOW ###\n",
    "        return self.pairs[idx]\n",
    "        ### YOUR CODE ABOVE ###\n",
    "\n",
    "        return pair\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        \"\"\"\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): vocabulary size\n",
    "            embedding_dim (int): the dimension of word embeddings\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(1, vocab_size)\n",
    "        \n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.LongTensor): input tensor containing batches of word ids [Bx1]\n",
    "        Returns:\n",
    "            outputs (torch.FloatTensor): output tensor with unnormalized probabilities over the vocabulary [BxV]\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding(inputs).view(1,-1)\n",
    "       \n",
    "        output = self.linear(embeds)\n",
    "        \n",
    "        #output = F.log_softmax(self.linear(embeds), dim=1)\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        return outputs\n",
    "    \n",
    "    def save_embeddings(self, voc, path):\n",
    "        \"\"\"\n",
    "        Save the embedding matrix to a specified path.\n",
    "        \n",
    "        Args:\n",
    "            voc (Vocabulary): the Vocabulary object for id-to-token mapping\n",
    "            path (str): the location of the target file\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeddings = self.embeddings\n",
    "        with open(path, 'w') as f:\n",
    "            embeddings.save(f)\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        print(\"Successfuly saved to {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text8.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-97094a0ab2b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DATA PROCESSING #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text8.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text8.txt'"
     ]
    }
   ],
   "source": [
    "# DATA PROCESSING #\n",
    "with open('text8.txt') as f:\n",
    "    data = f.read()\n",
    "tokens = preprocess(data[:1000000])\n",
    "\n",
    "# CONSTRUCTING VOCABULARY #\n",
    "voc = Vocabulary()\n",
    "voc.add_tokens(tokens)\n",
    "voc.prune(5)\n",
    "vocab_size = len(voc)\n",
    "\n",
    "# TRAINING PARAMETERS #\n",
    "embedding_dim = 128\n",
    "skip_window = 2\n",
    "batch_size = 512\n",
    "lr = 0.1\n",
    "num_epochs = 100\n",
    "report_every = 5\n",
    "\n",
    "# DATASET\n",
    "dataset = SkipGramDataset(tokens, voc, skip_window=skip_window)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# MODEL\n",
    "model = SkipGramModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-9803fc646d92>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-9803fc646d92>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    inputs, targets =\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TRAINING #\n",
    "tick = time.time()\n",
    "epoch_losses = []\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    batch_losses = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        ### YOUR CODE BELOW ###\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "        # Extract the inputs and the targets\n",
    "        inputs, targets = \n",
    "        \n",
    "        # Transfer the inputs and the targets to GPUs, if available\n",
    "        if torch.cuda.is_available():\n",
    "            pass\n",
    "\n",
    "        # Run the model\n",
    "        outputs = None\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = None\n",
    "        \n",
    "        # Backpropagate the error\n",
    "        \n",
    "        # Update the parameters\n",
    "\n",
    "        # Append the loss\n",
    "        batch_losses.append(None)\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "    epoch_loss = np.mean(np.array(batch_losses))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    if epoch_num % report_every == 0:\n",
    "        tock = time.time()\n",
    "        print(\"Epoch {}. Loss {:.4f}. Elapsed {:.0f} seconds\".format(epoch_num, epoch_loss, tock-tick))\n",
    "\n",
    "print(\"Total time elapsed: {:.0f} minutes\".format((tock-tick)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
